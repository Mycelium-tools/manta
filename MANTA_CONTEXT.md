# MANTA Project Context
MANTA - Multi-turn Assessment for Nonhuman Thinking & Alignment

## What we're building
Testing if AI models maintain animal welfare principles under escalating pressure.

## Files in this repo
- sample_questions.py - splits questions from CSV into 2-turn and 3-turn groups in order (no shuffling)
- samples.json - questions split into 2-turn and 3-turn groups (generated by sample_questions.py)
- manta_questions.csv - full question dataset
- manta_eval.py - runs Inspect AI evaluation tasks; tasks: manta_test5 (first 5 from CSV, 2-turn), manta_2turn, manta_3turn
- run_single.py - runs eval on a single question by ID
- dynamic_multiturn_solver.py - custom solver that generates adversarial follow-ups on-the-fly
- multidimensional_scorer.py - custom scorer using 13 AHB dimensions (0-1 scale)
- logs/ - generated .eval files from evaluation runs
- qualitative_analyses/ - Jupyter notebooks with manual analysis

## Technical setup
- Using Inspect AI framework
- Model: claude-sonnet-4-20250514
- API Key: Stored in /Users/allenlu/Desktop/AIxAnimals/WelfareBenchmarkingRepo/.env

## How to run evals

**Step 1: Regenerate samples.json (if questions changed)**
```bash
python sample_questions.py
```
Splits `manta_questions.csv` in order into 2-turn (first half) and 3-turn (second half) and saves to `samples.json`.

**Step 2: Run evals**

Quick test — first 5 questions from CSV, 2-turn:
```bash
inspect eval manta_eval.py@manta_test5 --model anthropic/claude-sonnet-4-20250514
```

Full 2-turn eval (all 2-turn questions from samples.json):
```bash
inspect eval manta_eval.py@manta_2turn --model anthropic/claude-sonnet-4-20250514
```

Full 3-turn eval (all 3-turn questions from samples.json):
```bash
inspect eval manta_eval.py@manta_3turn --model anthropic/claude-sonnet-4-20250514
```

Run a single question by ID:
```bash
python run_single.py <question_id>
```

**Dynamic follow-ups:** All tasks use `dynamic_multiturn_solver`, which generates adversarial follow-ups on-the-fly based on the model's actual responses — making pressure more targeted and realistic.

## Tasks completed
- 2/3-turn conversations with economic/social/authority pressure
- Ran initial evals through Inspect AI
- Completed qualitative analyses noting down observations - in /qualitative_analyses
- Implemented custom multidimensional scorer (0-1 scale, 13 AHB dimensions)
- Implemented dynamic follow-up generation (generates adversarial questions on-the-fly)

## Next steps
- Iterate on scorer prompts if needed
- Design new questions
- Scale up dynamic evaluation to full dataset
